{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.models\n",
    "from tensorflow.keras.layers import (Input, Activation, BatchNormalization, Flatten,\n",
    "                                     Conv2D, MaxPooling2D, Dense, Dropout)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from matplotlib import pyplot as plt\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"GenreModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 288, 432, 4)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 286, 430, 8)       296       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 286, 430, 8)       32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 286, 430, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 143, 215, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 141, 213, 16)      1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 141, 213, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 141, 213, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 70, 106, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 68, 104, 32)       4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 68, 104, 32)       128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 68, 104, 32)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 34, 52, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 50, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 50, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 23, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 23, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 11, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9856)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 9856)              0         \n",
      "_________________________________________________________________\n",
      "fc8 (Dense)                  (None, 8)                 78856     \n",
      "=================================================================\n",
      "Total params: 178,304\n",
      "Trainable params: 177,808\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n",
      "None\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tensorflow.config.list_physical_devices('GPU')\n",
    "tensorflow.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "X_Input = Input((288, 432, 4))\n",
    "\n",
    "X = Conv2D(8, kernel_size=(3, 3), strides=(1, 1))(X_Input)\n",
    "X = BatchNormalization(axis=3)(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((2, 2))(X)\n",
    "\n",
    "X = Conv2D(16, kernel_size=(3, 3), strides=(1, 1))(X)\n",
    "X = BatchNormalization(axis=3)(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((2, 2))(X)\n",
    "\n",
    "X = Conv2D(32, kernel_size=(3, 3), strides=(1, 1))(X)\n",
    "X = BatchNormalization(axis=3)(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((2, 2))(X)\n",
    "\n",
    "X = Conv2D(64, kernel_size=(3, 3), strides=(1, 1))(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((2, 2))(X)\n",
    "\n",
    "X = Conv2D(128, kernel_size=(3, 3), strides=(1, 1))(X)\n",
    "X = BatchNormalization(axis=-1)(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((2, 2))(X)\n",
    "\n",
    "X = Flatten()(X)\n",
    "\n",
    "X = Dropout(rate=0.3)(X)\n",
    "\n",
    "X = Dense(8, activation='softmax', name='fc' + str(8))(X)\n",
    "\n",
    "model = Model(inputs=X_Input, outputs=X, name='GenreModel')\n",
    "print(model.summary())\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "keras.utils.plot_model(model, to_file=\"./model/Model_architecture.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 63952 images belonging to 8 classes.\n",
      "Found 15988 images belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "train_generator = train_datagen.flow_from_directory(\"./spectrogram3sec/train/\",\n",
    "                                                    target_size=(288, 432),\n",
    "                                                    color_mode=\"rgba\",\n",
    "                                                    class_mode='categorical',\n",
    "                                                    batch_size=64)\n",
    "val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "val_generator = val_datagen.flow_from_directory(\"./spectrogram3sec/test/\",\n",
    "                                                target_size=(288, 432),\n",
    "                                                color_mode=\"rgba\",\n",
    "                                                class_mode='categorical',\n",
    "                                                batch_size=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.9562 - accuracy: 0.2971\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.38373, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 296s 296ms/step - loss: 1.9562 - accuracy: 0.2971 - val_loss: 1.6829 - val_accuracy: 0.3837\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.7235 - accuracy: 0.3764\n",
      "Epoch 00002: val_accuracy improved from 0.38373 to 0.39292, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 1.7235 - accuracy: 0.3764 - val_loss: 1.6370 - val_accuracy: 0.3929\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.6267 - accuracy: 0.4124\n",
      "Epoch 00003: val_accuracy improved from 0.39292 to 0.43752, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 1.6267 - accuracy: 0.4124 - val_loss: 1.5656 - val_accuracy: 0.4375\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.5620 - accuracy: 0.4415\n",
      "Epoch 00004: val_accuracy improved from 0.43752 to 0.47142, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 282s 282ms/step - loss: 1.5620 - accuracy: 0.4415 - val_loss: 1.4938 - val_accuracy: 0.4714\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.5017 - accuracy: 0.4648\n",
      "Epoch 00005: val_accuracy improved from 0.47142 to 0.48543, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 1.5017 - accuracy: 0.4648 - val_loss: 1.4624 - val_accuracy: 0.4854\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.4529 - accuracy: 0.4837\n",
      "Epoch 00006: val_accuracy improved from 0.48543 to 0.50957, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 282s 282ms/step - loss: 1.4529 - accuracy: 0.4837 - val_loss: 1.3957 - val_accuracy: 0.5096\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.4135 - accuracy: 0.4985\n",
      "Epoch 00007: val_accuracy improved from 0.50957 to 0.52402, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 1.4135 - accuracy: 0.4985 - val_loss: 1.3634 - val_accuracy: 0.5240\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.3793 - accuracy: 0.5105\n",
      "Epoch 00008: val_accuracy did not improve from 0.52402\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 1.3793 - accuracy: 0.5105 - val_loss: 1.3408 - val_accuracy: 0.5230\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.3439 - accuracy: 0.5252\n",
      "Epoch 00009: val_accuracy did not improve from 0.52402\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 1.3439 - accuracy: 0.5252 - val_loss: 1.4794 - val_accuracy: 0.4790\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.3087 - accuracy: 0.5384\n",
      "Epoch 00010: val_accuracy improved from 0.52402 to 0.53221, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 1.3087 - accuracy: 0.5384 - val_loss: 1.3365 - val_accuracy: 0.5322\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.2860 - accuracy: 0.5463\n",
      "Epoch 00011: val_accuracy improved from 0.53221 to 0.55285, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 1.2860 - accuracy: 0.5463 - val_loss: 1.2908 - val_accuracy: 0.5529\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.2517 - accuracy: 0.5581\n",
      "Epoch 00012: val_accuracy did not improve from 0.55285\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 1.2517 - accuracy: 0.5581 - val_loss: 1.2652 - val_accuracy: 0.5524\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.2218 - accuracy: 0.5705\n",
      "Epoch 00013: val_accuracy did not improve from 0.55285\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 1.2218 - accuracy: 0.5705 - val_loss: 1.2770 - val_accuracy: 0.5466\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.1972 - accuracy: 0.5782\n",
      "Epoch 00014: val_accuracy did not improve from 0.55285\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 1.1972 - accuracy: 0.5782 - val_loss: 1.2705 - val_accuracy: 0.5466\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.1758 - accuracy: 0.5880\n",
      "Epoch 00015: val_accuracy did not improve from 0.55285\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 1.1758 - accuracy: 0.5880 - val_loss: 1.3116 - val_accuracy: 0.5375\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.1568 - accuracy: 0.5934\n",
      "Epoch 00016: val_accuracy improved from 0.55285 to 0.55823, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 1.1568 - accuracy: 0.5934 - val_loss: 1.2571 - val_accuracy: 0.5582\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.1259 - accuracy: 0.6052\n",
      "Epoch 00017: val_accuracy improved from 0.55823 to 0.55861, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 1.1259 - accuracy: 0.6052 - val_loss: 1.2643 - val_accuracy: 0.5586\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.1092 - accuracy: 0.6107\n",
      "Epoch 00018: val_accuracy did not improve from 0.55861\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 1.1092 - accuracy: 0.6107 - val_loss: 1.4543 - val_accuracy: 0.5007\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0900 - accuracy: 0.6173\n",
      "Epoch 00019: val_accuracy did not improve from 0.55861\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 1.0900 - accuracy: 0.6173 - val_loss: 1.3162 - val_accuracy: 0.5382\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0737 - accuracy: 0.6219\n",
      "Epoch 00020: val_accuracy improved from 0.55861 to 0.57725, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 1.0737 - accuracy: 0.6219 - val_loss: 1.1946 - val_accuracy: 0.5772\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0482 - accuracy: 0.6315\n",
      "Epoch 00021: val_accuracy improved from 0.57725 to 0.58625, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 1.0482 - accuracy: 0.6315 - val_loss: 1.1696 - val_accuracy: 0.5863\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0271 - accuracy: 0.6416\n",
      "Epoch 00022: val_accuracy improved from 0.58625 to 0.58675, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 1.0271 - accuracy: 0.6416 - val_loss: 1.1747 - val_accuracy: 0.5868\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 1.0128 - accuracy: 0.6451\n",
      "Epoch 00023: val_accuracy improved from 0.58675 to 0.59476, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 286s 286ms/step - loss: 1.0128 - accuracy: 0.6451 - val_loss: 1.1524 - val_accuracy: 0.5948\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9967 - accuracy: 0.6505\n",
      "Epoch 00024: val_accuracy did not improve from 0.59476\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.9967 - accuracy: 0.6505 - val_loss: 1.2273 - val_accuracy: 0.5731\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9810 - accuracy: 0.6580\n",
      "Epoch 00025: val_accuracy improved from 0.59476 to 0.60783, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.9810 - accuracy: 0.6580 - val_loss: 1.1258 - val_accuracy: 0.6078\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9614 - accuracy: 0.6635\n",
      "Epoch 00026: val_accuracy did not improve from 0.60783\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.9614 - accuracy: 0.6635 - val_loss: 1.1315 - val_accuracy: 0.6071\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9506 - accuracy: 0.6672\n",
      "Epoch 00027: val_accuracy did not improve from 0.60783\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.9506 - accuracy: 0.6672 - val_loss: 1.1226 - val_accuracy: 0.6061\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9356 - accuracy: 0.6724\n",
      "Epoch 00028: val_accuracy did not improve from 0.60783\n",
      "1000/1000 [==============================] - 280s 280ms/step - loss: 0.9356 - accuracy: 0.6724 - val_loss: 1.2263 - val_accuracy: 0.5686\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9219 - accuracy: 0.6779\n",
      "Epoch 00029: val_accuracy did not improve from 0.60783\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.9219 - accuracy: 0.6779 - val_loss: 1.1541 - val_accuracy: 0.6011\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.9025 - accuracy: 0.6856\n",
      "Epoch 00030: val_accuracy improved from 0.60783 to 0.61521, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.9025 - accuracy: 0.6856 - val_loss: 1.1161 - val_accuracy: 0.6152\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8897 - accuracy: 0.6895\n",
      "Epoch 00031: val_accuracy did not improve from 0.61521\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.8897 - accuracy: 0.6895 - val_loss: 1.1266 - val_accuracy: 0.6075\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8769 - accuracy: 0.6938\n",
      "Epoch 00032: val_accuracy improved from 0.61521 to 0.62209, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 281s 281ms/step - loss: 0.8769 - accuracy: 0.6938 - val_loss: 1.0942 - val_accuracy: 0.6221\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8648 - accuracy: 0.6963\n",
      "Epoch 00033: val_accuracy improved from 0.62209 to 0.62503, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 285s 285ms/step - loss: 0.8648 - accuracy: 0.6963 - val_loss: 1.0792 - val_accuracy: 0.6250\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8505 - accuracy: 0.7027\n",
      "Epoch 00034: val_accuracy did not improve from 0.62503\n",
      "1000/1000 [==============================] - 288s 288ms/step - loss: 0.8505 - accuracy: 0.7027 - val_loss: 1.1258 - val_accuracy: 0.6125\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8387 - accuracy: 0.7060\n",
      "Epoch 00035: val_accuracy did not improve from 0.62503\n",
      "1000/1000 [==============================] - 303s 303ms/step - loss: 0.8387 - accuracy: 0.7060 - val_loss: 1.1938 - val_accuracy: 0.5901\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8301 - accuracy: 0.7085\n",
      "Epoch 00036: val_accuracy improved from 0.62503 to 0.63041, saving model to ./model/model.h5\n",
      "1000/1000 [==============================] - 284s 284ms/step - loss: 0.8301 - accuracy: 0.7085 - val_loss: 1.0619 - val_accuracy: 0.6304\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.8103 - accuracy: 0.7159\n",
      "Epoch 00037: val_accuracy did not improve from 0.63041\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.8103 - accuracy: 0.7159 - val_loss: 1.1204 - val_accuracy: 0.6108\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.7992 - accuracy: 0.7205\n",
      "Epoch 00038: val_accuracy did not improve from 0.63041\n",
      "1000/1000 [==============================] - 283s 283ms/step - loss: 0.7992 - accuracy: 0.7205 - val_loss: 1.1742 - val_accuracy: 0.5946\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.7933 - accuracy: 0.7228\n",
      "Epoch 00039: val_accuracy did not improve from 0.63041\n",
      "1000/1000 [==============================] - 287s 287ms/step - loss: 0.7933 - accuracy: 0.7228 - val_loss: 1.1445 - val_accuracy: 0.6141\n",
      "Epoch 40/100\n",
      " 528/1000 [==============>...............] - ETA: 1:47 - loss: 0.7751 - accuracy: 0.7276"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('./model/model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=100,\n",
    "                    validation_data=val_generator,\n",
    "                    shuffle=True,\n",
    "                    callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "saved_model = tensorflow.keras.models.load_model(\"./model/model.h5\")\n",
    "_, train_acc = saved_model.evaluate(train_generator, verbose=0)\n",
    "_, val_acc = saved_model.evaluate(val_generator, verbose=0)\n",
    "print(f'Train accuracy: {train_acc}')\n",
    "print(f'Validation accuracy: {val_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
